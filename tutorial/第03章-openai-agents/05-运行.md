您可以通过Runner类运行代理。您有三个选择：
Runner.run()，它异步运行并返回RunResult。
Runner.run_sync()，这是一个同步方法，只是在后台运行.run()。
Runner.run_streamed()，它异步运行并返回RunResultStreaming。它以流模式调用LLM，并在收到这些事件时将其流式传输给您。

这节用的程序的版本是：
mcp-1.9.4 openai-1.88.0 openai-agents-0.0.19 python-multipart-0.0.20

## 流式输出

Runner.run_sync()在第一课01-安装与配置就已经用过了。Runner.run()在前面的章节中也用过很多次。这里主要讲一下Runner.run_streamed()。我们可以把第一课里的那个例子用Runner.run_streamed()改造一下。

需要注意的是，Runner.run_streamed()前面不能用await，否则会报错：
TypeError: object RunResultStreaming can't be used in 'await' expression

```python
from agents import Agent, Runner, set_tracing_disabled
from agents.extensions.models.litellm_model import LitellmModel
from openai.types.responses import ResponseTextDeltaEvent
import os
import asyncio
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()
# 从环境变量中读取api_key
api_key = os.getenv('mistral_key')
base_url = 'https://api.mistral.ai/v1'
chat_model = "mistral/mistral-small-latest"
set_tracing_disabled(disabled=True)
llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)

agent = Agent(
    name="天气助手",
    instructions="始终用汉赋的形式回答用户",
    model=llm,
)

async def main():
    result = Runner.run_streamed(agent, "给我讲个程序员相亲的笑话")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            print(event.data.delta, end="", flush=True)


if __name__ == "__main__":
    asyncio.run(main())
```

运行程序后，会在终端看到流式输出以下内容：
昔有程序员，工巧擅写代码，日夜与编程作伴。然则情路坎坷，终须觅良缘，故赴相亲之会。

话说佳人对席，轻启朱唇，道：“君之喜好，可否细陈？”

程序员执酒，严词应对：“小生酷爱调试代码，若有错漏，修之快意；亦钟情算法，求其精妙。况且，代码乃吾之天下，键盘即吾之良友。”

佳人闻之，略感疑惑，复问：“若举一代码喻爱情，君将如何编写？”

程序员莞尔一笑，答曰：“若爱情可拟代码，吾当书‘if you love me == True: forever_partnership() else: heartbreak_emotion()’。无BUG，则永结同心。”

佳人拍案而笑，道：“君若心如此，吾有意参与你之程序，但代码之外，奈何将吾心动？”

程序员沉思良久，答曰：“吾之爱情如API，对外需兼容，对内当高效，愿汝作吾系统最重要之核心模块。”

言罢佳人，默然不语。原来程序员未曾谈笑风生，佳人却以为调试人生，误解于情，草草别去，留程序员独对屏幕，感叹：

“爱情如死循环，缘为何时能终止？”


在流式输出的数据中，event.type 不仅有 "raw_response_event"，还有"agent_updated_stream_event"、"run_item_stream_event"等等。我们都可以捕获到这些输出信息：

```python
import asyncio
import random
from agents.extensions.models.litellm_model import LitellmModel
from agents import Agent, ItemHelpers, Runner, function_tool, set_tracing_disabled
import os
import asyncio
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()
# 从环境变量中读取api_key
api_key = os.getenv('mistral_key')
base_url = 'https://api.mistral.ai/v1'
chat_model = "mistral/mistral-small-latest"
set_tracing_disabled(disabled=True)
llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)

@function_tool
def how_many_jokes() -> int:
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],
        model=llm,
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",
    )
    print("=== Run starting ===")

    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        # When the agent updates, print that
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        # When items are generated, print them
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types

    print("=== Run complete ===")


if __name__ == "__main__":
    asyncio.run(main())
```

运行上面代码，会输出：
=== Run starting ===
Agent updated: Joker
-- Message output:
 Hello! How can I assist you today?
=== Run complete ===

## 列表输入

当您在Runner中使用run方法时，您需要传入一个启动Agent和输入。输入可以是字符串（被视为用户消息），也可以是输入消息列表，这些消息是OpenAI Responses API中的消息。

下面这个例子中，我们传入的输入不是字符串，而是一个消息列表。
```python
import asyncio
from agents.extensions.models.litellm_model import LitellmModel
from agents import Agent, Runner, set_tracing_disabled
import os
import asyncio
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()
# 从环境变量中读取api_key
api_key = os.getenv('mistral_key')
base_url = 'https://api.mistral.ai/v1'
chat_model = "mistral/mistral-small-latest"
set_tracing_disabled(disabled=True)
llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)

agent = Agent(
    name="小助手",
    instructions="回答用户的问题。",
    model=llm,
)

messages = [
    {"role": "user", "content": "孔子的全名叫什么?"},
    {"role": "user", "content": "孟子的全名叫什么?"},
    ]

async def main():
    result = await Runner.run(
        agent,
        input=messages,
    )
    print(result.final_output)
if __name__ == "__main__":
    asyncio.run(main())
```

运行上述代码后，输出如下：
孔子的全名是**孔丘**，字**仲尼**。

孟子的全名是**孟轲**，字**子舆**。

## 连续多轮对话

您可以使用RunResultBase.to_input_list（）方法来获取上一个的输出，与本次的输入进行合并后，再送入Agent，就会看起来像连续对话一样。

```python
import asyncio
from agents.extensions.models.litellm_model import LitellmModel
from agents import Agent, Runner, set_tracing_disabled
import os
import asyncio
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()
# 从环境变量中读取api_key
api_key = os.getenv('mistral_key')
base_url = 'https://api.mistral.ai/v1'
chat_model = "mistral/mistral-small-latest"
set_tracing_disabled(disabled=True)
llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)

agent = Agent(name="Assistant", model=llm, instructions="Reply very concisely.")

async def main():
    # First turn
    result = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
    print(result.final_output)
    # San Francisco

    # Second turn
    new_input = result.to_input_list() + [{"role": "user", "content": "What state is it in?"}]
    result = await Runner.run(agent, new_input)
    print(result.final_output)
if __name__ == "__main__":
    asyncio.run(main())
```

运行上述代码，会输出：
San Francisco.
California.


## 终端的人类交互

run_demo_loop在循环中提示用户输入，保留回合之间的对话历史。默认情况下，它在生成模型输出时进行流式传输。键入quit或exit（或按Ctrl-D）退出循环。

```python
import asyncio
from agents.extensions.models.litellm_model import LitellmModel
from agents import Agent, Runner, set_tracing_disabled, run_demo_loop
import os
import asyncio
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()
# 从环境变量中读取api_key
api_key = os.getenv('mistral_key')
base_url = 'https://api.mistral.ai/v1'
chat_model = "mistral/mistral-small-latest"
set_tracing_disabled(disabled=True)
llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)
agent = Agent(name="Assistant", model=llm, instructions="Reply very concisely.")
async def main():
    await run_demo_loop(agent)
if __name__ == "__main__":
    asyncio.run(main())
```

运行程序后，在终端会有一个对话回合：
 > 孔子生活在什么时代？

[Agent updated: Assistant]
孔子生活在春秋时期。孔子生活在春秋时期。
 > 孟子呢？

[Agent updated: Assistant]
孟子生活在战国时期。孟子生活在战国时期。
 > 我呢？

[Agent updated: Assistant]
你生活在现代。你生活在现代。
 > exit