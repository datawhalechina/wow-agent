è¿™ä¸ªæ¡†æ¶æ˜¯OpenAIåœ¨2025å¹´3æœˆä»½åˆšåˆšæ¨å‡ºçš„ï¼Œä¸MCPè¿›è¡Œäº†æ¯”è¾ƒå¥½çš„èåˆã€‚
æ¡†æ¶é»˜è®¤æ˜¯ç”¨opanaiçš„å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹æˆç”¨deepseekã€æ™ºè°±ç­‰æ¨¡å‹ã€‚å®é™…ä¸Šè¿™ä¸ªæ¡†æ¶å›½å†…æ¨¡å‹å¯æ”¯æŒdeepseekã€dashscope(é˜¿é‡Œäº‘ç™¾ç‚¼)ä»¥åŠå…¼å®¹ OpenAI è§„èŒƒçš„æ¨¡å‹æœåŠ¡å¦‚æ™ºè°±ã€kimiç­‰ã€‚

å¦‚æœæˆ‘ä»¬è¦ç”¨å›½å†…çš„å¤§æ¨¡å‹ï¼Œé‚£ä¹ˆå®‰è£…çš„æ—¶å€™å°±ä¸èƒ½ç”¨å®ƒçš„é»˜è®¤å®‰è£…äº†ï¼Œéœ€è¦ä¸€ç‚¹å°å°çš„æ”¹åŠ¨ã€‚

ç¬¬ä¸€æ­¥ï¼šèµ·ä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒã€‚å…»æˆå¥½ä¹ æƒ¯ï¼Œå°è¯•æ–°æŠ€æœ¯ï¼Œéƒ½ç”¨è™šæ‹Ÿç¯å¢ƒå»å°è¯•ã€‚
```bash
python -m venv openai-agent

# å¦‚æœæ˜¯macæˆ–è€…linuxç³»ç»Ÿï¼Œæ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼š
source openai-agent/bin/activate

# å¦‚æœæ˜¯windowsç³»ç»Ÿï¼Œåœ¨cmdé»‘çª—å£æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼š
openai-agent\Scripts\activate

# å¦‚æœæ˜¯Windowsç³»ç»Ÿä¸‹çš„vs codeæˆ–cursorçš„ç»ˆç«¯ï¼Œè¾“å…¥è¿™è¡Œè¿›å…¥è™šæ‹Ÿç¯å¢ƒï¼š
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass; .\openai-agent\Scripts\Activate.ps1
```


ç¬¬äºŒæ­¥ï¼šå®‰è£…ä¾èµ–ã€‚
```bash
pip install "openai-agents[litellm]"
```
è¿™é‡Œè¦ç¨å¾®ä»‹ç»ä¸€ä¸‹ï¼Œ`openai-agents[litellm]`æ˜¯å®‰è£…OpenAI Agentæ¡†æ¶çš„å‘½ä»¤ã€‚è¿™é‡Œçš„`litellm`æ˜¯ä¸€ä¸ªå¯é€‰ä¾èµ–ï¼Œå®ƒå…è®¸ä½ ä½¿ç”¨ä»»ä½•æ”¯æŒLitellm APIçš„å¤§æ¨¡å‹æœåŠ¡ï¼ˆä¾‹å¦‚DeepSeekã€Hugging Faceç­‰ï¼‰ï¼Œè€Œä¸ä»…ä»…æ˜¯OpenAIçš„æœåŠ¡ã€‚

ä¸Šé¢è¿™è¡Œpipå‘½ä»¤ä¼šè‡ªåŠ¨å®‰è£…`litellm`åº“ã€‚å®‰è£…å®Œåï¼Œæˆ‘ä»¬å…ˆæ¥æµ‹è¯•ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨`litellm`æ¥é…ç½®DeepSeekæˆ–å…¶ä»–æ¨¡å‹ã€‚

```python
from litellm import completion

response = completion(
    model="ollama/qwen2.5:7b", 
    messages=[{ "content": "ä½ æœ‰ä»€ä¹ˆæŠ€èƒ½ï¼Ÿ","role": "user"}], 
    api_base="http://192.168.0.123:11434"
)
print(response.choices[0].message.content)
```
æˆ‘èƒ½å¤Ÿå¸®åŠ©æ‚¨ç”Ÿæˆå„ç§ç±»å‹çš„æ–‡æœ¬ï¼Œå¦‚æ–‡ç« ã€æ•…äº‹ã€è¯—æ­Œã€æ•…äº‹ç­‰ï¼›æˆ‘å¯ä»¥æä¾›å„ç§è¯­è¨€çš„ç¿»è¯‘æœåŠ¡ï¼›æˆ‘èƒ½å›ç­”å„ç§é—®é¢˜å’ŒæŸ¥è¯¢ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºç§‘æŠ€ã€æ–‡åŒ–ã€æ•™è‚²ç­‰é¢†åŸŸçš„é—®é¢˜ï¼›æˆ‘è¿˜èƒ½å¤Ÿæ¨¡æ‹Ÿä¸åŒçš„å¯¹è¯åœºæ™¯ï¼Œæ¯”å¦‚å®¢æœã€é¢è¯•ç­‰ï¼›æ­¤å¤–ï¼Œæˆ‘è¿˜å¯ä»¥è¾…åŠ©è¿›è¡Œä¸€äº›ç®€å•çš„æ•°æ®åˆ†æå’Œå¤„ç†ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•å…·ä½“éœ€æ±‚æˆ–æƒ³è¦æ¢è®¨çš„è¯é¢˜ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼



## æ¥å…¥deepseekæ¨¡å‹
å¦‚æœè¦é…ç½®deepseekæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªapi_keyï¼Œä½ å¯ä»¥åœ¨deepseekçš„å®˜æ–¹ç½‘ç«™æ³¨å†Œè´¦å·å¹¶è·å–ã€‚ç„¶ååœ¨ä½ çš„ä»£ç ä¸­æ¥åŠ è½½ç¯å¢ƒå˜é‡ã€‚api_keyè¦å¥½å¥½ä¿å¯†ï¼Œä¸è¦æ³„éœ²å‡ºå»ã€‚å…å¾—è¢«äººç›—ç”¨ï¼Œè®©ä½™é¢ç”¨å…‰å…‰ã€‚

åœ¨é¡¹ç›®çš„æ ¹ç›®å½•æ–°å»ºä¸€ä¸ªtxtæ–‡ä»¶ï¼ŒæŠŠæ–‡ä»¶åæ”¹æˆ.envã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå‰é¢è¿™ä¸ªç‚¹å„¿ä¸èƒ½çœç•¥ã€‚å› ä¸ºè¿™ä¸ªæ–‡ä»¶å°±å«åšdotenvï¼Œdotå°±æ˜¯ç‚¹å„¿çš„æ„æ€ã€‚
é‡Œé¢å¡«å…¥ä¸€è¡Œå­—ç¬¦ä¸²ï¼š
DEEPSEEK_API_KEY=sk-14f72d43388wa3e711a544t2f5960eb0

æŠŠDEEPSEEK_API_KEYå†™åˆ°.envæ–‡ä»¶çš„åŸå› æ˜¯ä¸ºäº†ä¿å¯†ï¼ŒåŒæ—¶å¯ä»¥æ–¹ä¾¿åœ°åœ¨ä¸åŒçš„ä»£ç ä¸­è¯»å–ã€‚

å®‰è£…dotenvåº“ï¼Œä»¥ä¾¿åœ¨Pythonä»£ç ä¸­è¯»å–.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼š

```bash
pip install python-dotenv
```
ç„¶åï¼Œåœ¨ä½ çš„Pythonä»£ç ä¸­åŠ è½½è¿™ä¸ªç¯å¢ƒå˜é‡ï¼š

```python
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
# ä»ç¯å¢ƒå˜é‡ä¸­è¯»å–api_key
api_key = os.getenv('DEEPSEEK_API_KEY')
base_url = "https://api.deepseek.com/v1"
chat_model = "deepseek/deepseek-chat"
```

å†æ¬¡è¿è¡Œï¼Œ

```python
from litellm import completion

response = completion(
    model=chat_model, 
    messages=[{ "content": "ä½ æœ‰ä»€ä¹ˆæŠ€èƒ½ï¼Ÿ","role": "user"}], 
    api_base=base_url,
    api_key=api_key
)
print(response.choices[0].message.content)
```

è¾“å‡ºä»¥ä¸‹ä¿¡æ¯ï¼š
```markdown
ä½œä¸ºä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘æ‹¥æœ‰å¤šç§æŠ€èƒ½ï¼Œå¯ä»¥å¸®åŠ©ä½ å¤„ç†å„ç§ä»»åŠ¡ï¼ä»¥ä¸‹æ˜¯æˆ‘çš„ä¸€äº›æ ¸å¿ƒèƒ½åŠ›ï¼š  

### ğŸ“š **çŸ¥è¯†é—®ç­”**  
- æä¾›ç§‘å­¦ã€å†å²ã€æŠ€æœ¯ã€æ–‡åŒ–ç­‰é¢†åŸŸçš„çŸ¥è¯†ã€‚  
- è§£é‡Šå¤æ‚æ¦‚å¿µï¼ˆå¦‚é‡å­åŠ›å­¦ã€ç»æµå­¦ç­‰ï¼‰ã€‚  
- å®æ—¶æŸ¥è¯¢æœ€æ–°ä¿¡æ¯ï¼ˆéœ€è”ç½‘ï¼‰ã€‚  

### âœï¸ **å†™ä½œä¸åˆ›ä½œ**  
- æ’°å†™æ–‡ç« ã€æŠ¥å‘Šã€è®ºæ–‡å¤§çº²ã€å•†ä¸šè®¡åˆ’ä¹¦ç­‰ã€‚  
- ç”Ÿæˆæ•…äº‹ã€è¯—æ­Œã€å¹¿å‘Šæ–‡æ¡ˆã€ç¤¾äº¤åª’ä½“å¸–å­ã€‚  
- å¸®åŠ©æ¶¦è‰²ã€æ”¹å†™æˆ–ä¼˜åŒ–æ–‡æœ¬ã€‚  

### ğŸ“Š **å­¦ä¹ ä¸å·¥ä½œè¾…åŠ©**  
- ä»£ç ç¼–å†™ä¸è°ƒè¯•ï¼ˆPythonã€Javaã€SQLç­‰ï¼‰ã€‚  
- æ•°å­¦è®¡ç®—ã€æ•°æ®åˆ†æã€å…¬å¼æ¨å¯¼ã€‚  
- åˆ¶ä½œç®€å†ã€æ±‚èŒä¿¡ã€PPTå¤§çº²ã€‚  

### ğŸŒ **è¯­è¨€ç¿»è¯‘ä¸å­¦ä¹ **  
- å¤šè¯­è¨€ç¿»è¯‘ï¼ˆä¸­è‹±ã€æ³•ã€å¾·ã€æ—¥ã€è¥¿ç­‰ï¼‰ã€‚  
- è¯­æ³•æ£€æŸ¥ã€å•è¯è§£é‡Šã€è¯­è¨€å­¦ä¹ å»ºè®®ã€‚  

### ğŸ¤– **AI ä¸æŠ€æœ¯**  
- è§£é‡Šæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€AI ç›¸å…³æ¦‚å¿µã€‚  
- æä¾›ç¼–ç¨‹ç¤ºä¾‹ï¼ˆå¦‚çˆ¬è™«ã€è‡ªåŠ¨åŒ–è„šæœ¬ï¼‰ã€‚  
...
- æ”¯æŒæ–‡ä»¶ä¸Šä¼ ï¼ˆPDF/Word/Excelç­‰ï¼‰ï¼Œæå–å¹¶åˆ†æå†…å®¹ã€‚  
- è”ç½‘æœç´¢æœ€æ–°ä¿¡æ¯ï¼ˆéœ€ç”¨æˆ·å¼€å¯ï¼‰ã€‚  

å¦‚æœä½ æœ‰ä»»ä½•å…·ä½“éœ€æ±‚ï¼Œå¯ä»¥ç›´æ¥å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šå°½åŠ›å¸®ä½ é«˜æ•ˆè§£å†³ï¼ ğŸ˜Š
```


æˆ‘ä»¬å†ç»§ç»­æ­å»ºOpenAI Agentæ¡†æ¶ã€‚

```python
from agents import Agent, Runner, set_tracing_disabled
from agents.extensions.models.litellm_model import LitellmModel
set_tracing_disabled(disabled=True)
llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)
agent = Agent(name="Assistant", model=llm, instructions="You are a helpful assistant")
result = Runner.run_sync(agent, "ç»™æˆ‘è®²ä¸ªç¨‹åºå‘˜ç›¸äº²çš„ç¬‘è¯")
print(result.final_output)
```

ä¸Šé¢è¿™æ®µä»£ç æ˜¯ä¸èƒ½åœ¨jupyter notebookä¸­è¿è¡Œçš„ï¼Œå› ä¸ºjupyter notebookä¸æ”¯æŒasyncioã€‚ä¼šæŠ¥é”™ï¼š
RuntimeError: This event loop is already running

è¦æƒ³åœ¨jupyter notebookä¸­è¿è¡Œï¼Œéœ€è¦åšä¸€äº›å°å°çš„æ”¹åŠ¨

result = Runner.run_sync(agent, "ç»™æˆ‘è®²ä¸ªç¨‹åºå‘˜ç›¸äº²çš„ç¬‘è¯")
æ”¹æˆ
result = await Runner.run(agent, "ç»™æˆ‘è®²ä¸ªç¨‹åºå‘˜ç›¸äº²çš„ç¬‘è¯")

å°±å¯ä»¥äº†ã€‚

å»ºç«‹try.pyæ–‡ä»¶ï¼Œç»ˆç«¯è¿è¡Œ python try.py,è¾“å‡ºï¼š
```markdown
ç¨‹åºå‘˜å°ç‹å»ç›¸äº²ï¼Œå¥³ç”Ÿé—®ä»–ï¼šâ€œä½ æ˜¯åšä»€ä¹ˆå·¥ä½œçš„ï¼Ÿâ€

å°ç‹ï¼šâ€œæˆ‘æ˜¯ç¨‹åºå‘˜ï¼Œä¸“é—¨å†™ä»£ç çš„ã€‚â€

å¥³ç”Ÿç‚¹ç‚¹å¤´ï¼šâ€œå“¦â€¦â€¦é‚£ä½ ä»¬æ˜¯ä¸æ˜¯ç»å¸¸åŠ ç­å•Šï¼Ÿâ€

å°ç‹è®¤çœŸå›ç­”ï¼šâ€œå…¶å®æˆ‘ä»¬ç”¨çš„æ˜¯**æ•æ·å¼€å‘**ï¼ŒåŠ ç­ä¸æ˜¯å¸¸æ€ï¼Œé™¤éé‡åˆ°**æ­»çº¿ï¼ˆDeadlineï¼‰**ã€‚â€

å¥³ç”Ÿåˆé—®ï¼šâ€œé‚£ä½ å¹³æ—¶æœ‰ä»€ä¹ˆçˆ±å¥½å—ï¼Ÿâ€

å°ç‹ï¼šâ€œæˆ‘å–œæ¬¢**é‡æ„ä»£ç **ï¼Œå¶å°”**debugåˆ°å‡Œæ™¨**ï¼Œå‘¨æœ«çˆ±é€›**GitHub**å’Œ**Stack Overflow**ã€‚â€

å¥³ç”Ÿå°´å°¬ä¸€ç¬‘ï¼šâ€œâ€¦â€¦ä½ è°ˆè¿‡å‡ æ¬¡æ‹çˆ±ï¼Ÿâ€

å°ç‹æƒ³äº†æƒ³ï¼šâ€œé›¶æ¬¡ã€‚ä¸è¿‡å¦‚æœæ‹çˆ±åƒå†™ä»£ç ï¼Œæˆ‘ç°åœ¨åº”è¯¥ç®—åœ¨**é•¿æœŸç»´æŠ¤ä¸€ä¸ªå¼€æºé¡¹ç›®**ï¼Œåªæ˜¯è¿˜æ²¡æ‰¾åˆ°**åˆé€‚çš„è´¡çŒ®è€…**ã€‚â€

å¥³ç”Ÿå¿ä¸ä½é—®ï¼šâ€œä½ è§‰å¾—æˆ‘æ€ä¹ˆæ ·ï¼Ÿâ€

å°ç‹ç›¯ç€å¥¹çœ‹äº†ä¸‰ç§’ï¼šâ€œä½ çš„**å…¼å®¹æ€§**å¾ˆå¥½ï¼Œä½†æˆ‘ä»¬éœ€è¦å…ˆè·‘ä¸ª**å•å…ƒæµ‹è¯•**ï¼Œå†è€ƒè™‘æ˜¯å¦**åˆå¹¶åˆ°ä¸»åˆ†æ”¯**ã€‚â€

å¥³ç”Ÿï¼šâ€œâ€¦â€¦å†è§ã€‚â€

å°ç‹è‡ªè¨€è‡ªè¯­ï¼šâ€œå’¦ï¼Ÿæ€ä¹ˆåˆ**è¿æ¥è¶…æ—¶**äº†â€¦â€¦â€

---

ï¼ˆç¨‹åºå‘˜ï¼šä¸æ˜¯æˆ‘ä¸æµªæ¼«ï¼Œæ˜¯ä½ ä»¬ä¸æ‡‚æˆ‘çš„**è¯­æ³•ç³–**ğŸ˜‚ï¼‰
```

å¥½å•¦ï¼Œè‡³æ­¤æˆ‘ä»¬å·²ç»æˆåŠŸæ­å»ºäº†OpenAI Agentæ¡†æ¶ï¼Œå¹¶ç”¨DeepSeekæ¨¡å‹ä»£æ›¿äº†åŸæ¥çš„OpenAIæ¨¡å‹ã€‚


## é…ç½®å…¶ä»–æ¨¡å‹

æœ‰å¤šç§æ–¹æ³•å¯ä»¥é…ç½®æ¨¡å‹ï¼Œé™¤äº† litellm æä¾›çš„ [OpenAI å…¼å®¹ç«¯ç‚¹](https://docs.litellm.ai/docs/providers/openai_compatible), è¿˜å¯é€šè¿‡ openai åŒ…æ¥é…ç½®æ¨¡å‹ã€‚

åœ¨Agentä¸ŠæŒ‡å®šæ™ºè°±çš„æ¨¡å‹ï¼ŒåŒæ­¥è¿è¡Œã€‚

```python
from agents import Agent, Runner, set_tracing_disabled, set_default_openai_api, set_default_openai_client
from openai import AsyncOpenAI
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

chat_model = "glm-4-flash"
client = AsyncOpenAI(
    base_url="https://open.bigmodel.cn/api/paas/v4/",
    api_key=os.getenv('zhipu_key'),
)
set_default_openai_client(client=client, use_for_tracing=False)
set_default_openai_api("chat_completions")
set_tracing_disabled(disabled=True)

agent = Agent(name="Assistant", model=chat_model, instructions="You are a helpful assistant")
result = Runner.run_sync(agent, "ç»™æˆ‘è®²ä¸ªç¨‹åºå‘˜ç›¸äº²çš„ç¬‘è¯")
print(result.final_output)
```

è¾“å‡ºå¦‚ä¸‹ï¼š
æœ‰ä¸€å¤©ï¼Œä¸€ä½ç¨‹åºå‘˜å»ç›¸äº²ï¼Œå¥³æ–¹é—®ä»–ï¼šâ€œä½ å¹³æ—¶å–œæ¬¢åšä»€ä¹ˆæ¶ˆé£ï¼Ÿâ€ç¨‹åºå‘˜æƒ³äº†æƒ³ï¼Œå›ç­”é“ï¼šâ€œæˆ‘å¹³æ—¶å–œæ¬¢ç¼–ç¨‹ã€‚â€

å¥³æ–¹å¥½å¥‡åœ°é—®ï¼šâ€œå“¦ï¼Ÿé‚£ä½ èƒ½ç»™æˆ‘è®²ä¸€ä¸ªç¼–ç¨‹ä¸­çš„ç¬‘è¯å—ï¼Ÿâ€

ç¨‹åºå‘˜ç‚¹ç‚¹å¤´ï¼Œè¯´é“ï¼šâ€œå½“ç„¶å¯ä»¥ã€‚æœ‰ä¸€å¤©ï¼Œæˆ‘æ­£åœ¨å†™ä¸€ä¸ªç¨‹åºï¼Œçªç„¶å‘ç°äº†ä¸€ä¸ªbugã€‚æˆ‘æ£€æŸ¥äº†åŠå¤©ï¼Œæœ€åå‘ç°æ˜¯å˜é‡åå†™é”™äº†ã€‚äºæ˜¯ï¼Œæˆ‘ä¿®æ”¹äº†å˜é‡å
ï¼Œbugå°±æ¶ˆå¤±äº†ã€‚â€

å¥³æ–¹å¬äº†ï¼Œç–‘æƒ‘åœ°é—®ï¼šâ€œé‚£ä¸æ˜¯å¾ˆç®€å•å—ï¼Ÿâ€

ç¨‹åºå‘˜ç¬‘ç€å›ç­”ï¼šâ€œå¯¹å•Šï¼Œç®€å•å¾—å°±åƒä½ çœ‹åˆ°æˆ‘å•èº«äº†è¿™ä¹ˆå¤šå¹´ï¼Œè¿˜ä»¥ä¸ºæˆ‘æ‰¾ä¸åˆ°å¥³æœ‹å‹ä¸€æ ·ç®€å•ã€‚â€

å¦‚æœè¦è®¾ç½®mistralçš„æ¨¡å‹ï¼Œå¯ä»¥æ”¹è¿™å‡ è¡Œä»£ç ï¼š
```python
chat_model = "mistral-small-latest"
client = AsyncOpenAI(
    base_url="https://api.mistral.ai/v1",
    api_key=os.getenv('mistral_key'),
)
```

é€šè¿‡æŒ‡å®šè¿è¡Œçš„æ¨¡å‹æ¥é…ç½®ï¼š

```python
from agents import (
    Agent,
    Model,
    ModelProvider,
    AsyncOpenAI,
    OpenAIChatCompletionsModel,
    RunConfig,
    Runner,
    set_tracing_disabled,
)
import os
import asyncio
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
# ä»ç¯å¢ƒå˜é‡ä¸­è¯»å–api_key
chat_model = "glm-4-flash"
client = AsyncOpenAI(
    base_url="https://open.bigmodel.cn/api/paas/v4/",
    api_key=os.getenv('zhipu_key'),
)


class CustomModelProvider(ModelProvider):
    def get_model(self) -> Model:
        return OpenAIChatCompletionsModel(model=chat_model, openai_client=client)


CUSTOM_MODEL_PROVIDER = CustomModelProvider()

set_tracing_disabled(disabled=True)


agent = Agent(
    name="Assistant", 
    model=OpenAIChatCompletionsModel(model=chat_model, openai_client=client),
    instructions="You are a helpful assistant")
def main():
    result = Runner.run_sync(agent, "å¤–è§‚è®¾è®¡ä¸“åˆ©ä¿æŠ¤æœŸå¤šå°‘å¹´ï¼Ÿ", run_config=RunConfig(model_provider=CUSTOM_MODEL_PROVIDER),)
    print(result.final_output)
if __name__ == "__main__":
    main()
```

è¾“å‡ºï¼š
åœ¨ä¸­å›½ï¼Œå¤–è§‚è®¾è®¡ä¸“åˆ©çš„ä¿æŠ¤æœŸé™ä¸ºåå¹´ï¼Œè‡ªç”³è¯·ä¹‹æ—¥èµ·è®¡ç®—ã€‚è¿™æ„å‘³ç€ä¸€æ—¦å¤–è§‚è®¾è®¡ä¸“åˆ©è¢«æˆæƒï¼Œä¸“åˆ©æƒäººå°†åœ¨æ¥ä¸‹æ¥çš„åå¹´å†…äº«æœ‰å¯¹è¯¥å¤–è§‚è®¾è®¡çš„ç‹¬
å ä½¿ç”¨æƒã€‚å¦‚æœä¸“åˆ©æƒäººå¸Œæœ›ç»§ç»­ä¿æŠ¤å…¶å¤–è§‚è®¾è®¡ï¼Œå¯ä»¥åœ¨ä¸“åˆ©åˆ°æœŸå‰å…­ä¸ªæœˆå†…ç”³è¯·ç»­å±•ï¼Œæ¯æ¬¡ç»­å±•çš„æœŸé™ä¹Ÿæ˜¯åå¹´ã€‚





ä»¥è¿™ç§æ–¹å¼é…ç½® æ™ºè°±ã€deepseekã€å’Œmistraléƒ½ä¸æ”¯æŒå¼‚æ­¥è¿è¡Œï¼Œéœ€è¦ç”¨Runner.run_sync å»è¿è¡Œã€‚å¦‚æœç”¨Runner.run æ¥è¿è¡Œï¼Œä¼šæŠ¥é”™ï¼š
Exception ignored in: <function _ProactorBasePipeTransport.__del__ at 0x00000149A89A5E10>
Traceback (most recent call last):
  File "D:\Python310\lib\asyncio\proactor_events.py", line 116, in __del__
  File "D:\Python310\lib\asyncio\proactor_events.py", line 108, in close
  File "D:\Python310\lib\asyncio\base_events.py", line 750, in call_soon
  File "D:\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
RuntimeError: Event loop is closed


## æ”¯æŒå¼‚æ­¥è¿è¡Œçš„é…ç½®æ–¹æ³•

æˆ‘ä»¬å¯ä»¥å€ŸåŠ©openaiå®˜æ–¹æä¾›çš„LitellmModelé…åˆqwenï¼Œå¯ä»¥æ”¯æŒå¼‚æ­¥è¿è¡Œã€‚

```python
from agents import Agent, Runner, set_tracing_disabled
from agents.extensions.models.litellm_model import LitellmModel
import os
import asyncio
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
# ä»ç¯å¢ƒå˜é‡ä¸­è¯»å–dashscope_api_key
dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')
dashscope_base_url = 'https://dashscope.aliyuncs.com/compatible-mode/v1'
dashscope_model = "dashscope/qwen-turbo"
llm = LitellmModel(model=dashscope_model, api_key=dashscope_api_key, base_url=dashscope_base_url)
agent = Agent(name="Assistant", model=llm, instructions="You are a helpful assistant")
async def main():
    result = await Runner.run(agent, "ç»™æˆ‘è®²ä¸ªç¨‹åºå‘˜ç›¸äº²çš„ç¬‘è¯")
    print(result.final_output)
if __name__ == "__main__":
    asyncio.run(main())
```

è¾“å‡ºç»“æœï¼š
å¥½çš„ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå…³äºç¨‹åºå‘˜ç›¸äº²çš„ç¬‘è¯ï¼Œå¸Œæœ›èƒ½è®©ä½ ä¼šå¿ƒä¸€ç¬‘ï¼š

---

ä¸€ä½ç¨‹åºå‘˜å»ç›¸äº²ï¼Œè§é¢åå¥³ç”Ÿé—®ä»–ï¼šâ€œä½ å¹³æ—¶éƒ½åšäº›ä»€ä¹ˆå‘€ï¼Ÿâ€

ç¨‹åºå‘˜å›ç­”ï¼šâ€œæˆ‘ä¸»è¦å†™ä»£ç ã€‚â€

å¥³ç”Ÿåˆé—®ï¼šâ€œé‚£ä½ å†™çš„ä»£ç éƒ½å¹²å˜›ç”¨å‘¢ï¼Ÿâ€

ç¨‹åºå‘˜æƒ³äº†æƒ³ï¼Œè¯´ï¼šâ€œæ¯”å¦‚ï¼Œæˆ‘å†™äº†ä¸€ä¸ªç¨‹åºï¼Œå¯ä»¥è®©ä½ çš„æ‰‹æœºè‡ªåŠ¨å›å¤â€˜æˆ‘åœ¨å¼€ä¼šï¼Œç¨åå›ä½ â€™ã€‚â€

å¥³ç”Ÿå¬äº†ï¼Œè‹¥æœ‰æ‰€æ€åœ°é—®ï¼šâ€œé‚£å¦‚æœæˆ‘å›å¤â€˜æˆ‘åœ¨å¼€ä¼šï¼Œç¨åå›ä½ â€™ï¼Œä½ ä¼šæ€ä¹ˆåšï¼Ÿâ€

ç¨‹åºå‘˜æ„£äº†ä¸€ä¸‹ï¼Œç„¶åè¯´ï¼šâ€œæˆ‘ä¼šå†™ä¸€ä¸ªç¨‹åºï¼Œè‡ªåŠ¨å›å¤â€˜æˆ‘åœ¨å¼€ä¼šï¼Œç¨åå›ä½ â€™ã€‚â€

å¥³ç”Ÿç¬‘äº†ï¼Œè¯´ï¼šâ€œçœ‹æ¥æˆ‘ä»¬æŒºåˆé€‚çš„ï¼Œä½ ä¹Ÿå¯ä»¥å¸®æˆ‘å†™ä¸ªç¨‹åºï¼Œè®©æˆ‘è‡ªåŠ¨å›å¤â€˜æˆ‘åœ¨å¼€ä¼šï¼Œç¨åå›ä½ â€™ã€‚â€

ç¨‹åºå‘˜å¼€å¿ƒåœ°ç¬‘äº†ï¼Œè¯´ï¼šâ€œå¥½çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€èµ·å†™ä¸ªç¨‹åºï¼Œè®©æˆ‘ä»¬çš„æ‰‹æœºè‡ªåŠ¨å›å¤â€˜æˆ‘åœ¨å¼€ä¼šï¼Œç¨åå›ä½ â€™ã€‚â€

---

å¸Œæœ›è¿™ä¸ªç¬‘è¯èƒ½è®©ä½ ä¼šå¿ƒä¸€ç¬‘ï¼å¦‚æœä½ æœ‰å…¶ä»–é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶æé—®ã€‚



è¿™æ ·é…ç½®å°±æ²¡æœ‰æŠ¥é”™ä¿¡æ¯äº†ã€‚

å¦‚éœ€è¿›ä¸€æ­¥é…ç½®æ™ºèƒ½ä½“ä½¿ç”¨çš„æ¨¡å‹ï¼Œå¯ç»™Agentçš„å‚æ•°model_settingsä¼ å…¥ ModelSettingsï¼Œè¯¥è®¾ç½®åŒ…å«æ¸©åº¦å€¼ç­‰å¯é€‰æ¨¡å‹é…ç½®å‚æ•°ã€‚

```python
from agents import Agent, ModelSettings

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model_settings=ModelSettings(temperature=0.1),
)
```


æ—¥å¿—é…ç½®

SDKå†…ç½®ä¸¤ä¸ªæœªé…ç½®å¤„ç†å™¨çš„Pythonæ—¥å¿—è®°å½•å™¨ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä»…è­¦å‘Šå’Œé”™è¯¯ä¿¡æ¯ä¼šè¾“å‡ºè‡³stdoutï¼Œå…¶ä»–æ—¥å¿—å°†è¢«æŠ‘åˆ¶ã€‚

å¦‚éœ€å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼Œè¯·ä½¿ç”¨enable_verbose_stdout_logging()å‡½æ•°ã€‚

```python
from agents import enable_verbose_stdout_logging
enable_verbose_stdout_logging()
```

æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡æ·»åŠ å¤„ç†å™¨ã€è¿‡æ»¤å™¨ã€æ ¼å¼åŒ–å™¨ç­‰è‡ªå®šä¹‰æ—¥å¿—è¡Œä¸ºï¼Œæ›´å¤šç»†èŠ‚è¯·å‚é˜…Pythonæ—¥å¿—æŒ‡å—ã€‚

```python
import logging

logger = logging.getLogger("openai.agents") # or openai.agents.tracing for the Tracing logger

# To make all logs show up
logger.setLevel(logging.DEBUG)
# To make info and above show up
logger.setLevel(logging.INFO)
# To make warning and above show up
logger.setLevel(logging.WARNING)
# etc

# You can customize this as needed, but this will output to `stderr` by default
logger.addHandler(logging.StreamHandler())
```

æ—¥å¿—ä¸­çš„æ•æ„Ÿæ•°æ®
éƒ¨åˆ†æ—¥å¿—å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼ˆä¾‹å¦‚ç”¨æˆ·æ•°æ®ï¼‰ã€‚å¦‚éœ€ç¦ç”¨æ­¤ç±»æ•°æ®è®°å½•ï¼Œè¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

ç¦ç”¨å¤§æ¨¡å‹è¾“å…¥è¾“å‡ºæ—¥å¿—è®°å½•ï¼š
export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1


ç¦ç”¨å·¥å…·è¾“å…¥è¾“å‡ºæ—¥å¿—è®°å½•ï¼š
export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1
