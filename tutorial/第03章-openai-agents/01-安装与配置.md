这个框架是OpenAI在2025年3月份刚刚推出的，与MCP进行了比较好的融合。
框架默认是用opanai的大模型，我们可以改成用deepseek、智谱等模型。实际上这个框架国内模型可支持deepseek、dashscope(阿里云百炼)以及兼容 OpenAI 规范的模型服务如智谱、kimi等。

如果我们要用国内的大模型，那么安装的时候就不能用它的默认安装了，需要一点小小的改动。

第一步：起一个虚拟环境。养成好习惯，尝试新技术，都用虚拟环境去尝试。
```bash
python -m venv openai-agent

# 如果是mac或者linux系统，激活虚拟环境：
source openai-agent/bin/activate

# 如果是windows系统，在cmd黑窗口激活虚拟环境：
openai-agent\Scripts\activate

# 如果是Windows系统下的vs code或cursor的终端，输入这行进入虚拟环境：
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass; .\openai-agent\Scripts\Activate.ps1
```


第二步：安装依赖。
```bash
pip install "openai-agents[litellm]"
```
这里要稍微介绍一下，`openai-agents[litellm]`是安装OpenAI Agent框架的命令。这里的`litellm`是一个可选依赖，它允许你使用任何支持Litellm API的大模型服务（例如DeepSeek、Hugging Face等），而不仅仅是OpenAI的服务。

上面这行pip命令会自动安装`litellm`库。安装完后，我们先来测试一下如何使用`litellm`来配置DeepSeek或其他模型。

```python
from litellm import completion

response = completion(
    model="ollama/qwen2.5:7b", 
    messages=[{ "content": "你有什么技能？","role": "user"}], 
    api_base="http://192.168.0.123:11434"
)
print(response.choices[0].message.content)
```
我能够帮助您生成各种类型的文本，如文章、故事、诗歌、故事等；我可以提供各种语言的翻译服务；我能回答各种问题和查询，包括但不限于科技、文化、教育等领域的问题；我还能够模拟不同的对话场景，比如客服、面试等；此外，我还可以辅助进行一些简单的数据分析和处理。如果您有任何具体需求或想要探讨的话题，请随时告诉我！



## 接入deepseek模型
如果要配置deepseek模型，我们需要一个api_key，你可以在deepseek的官方网站注册账号并获取。然后在你的代码中来加载环境变量。api_key要好好保密，不要泄露出去。免得被人盗用，让余额用光光。

在项目的根目录新建一个txt文件，把文件名改成.env。需要注意的是，前面这个点儿不能省略。因为这个文件就叫做dotenv，dot就是点儿的意思。
里面填入一行字符串：
DEEPSEEK_API_KEY=sk-14f72d43388wa3e711a544t2f5960eb0

把DEEPSEEK_API_KEY写到.env文件的原因是为了保密，同时可以方便地在不同的代码中读取。

安装dotenv库，以便在Python代码中读取.env文件中的环境变量：

```bash
pip install python-dotenv
```
然后，在你的Python代码中加载这个环境变量：

```python
import os
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()
# 从环境变量中读取api_key
api_key = os.getenv('DEEPSEEK_API_KEY')
base_url = "https://api.deepseek.com/v1"
chat_model = "deepseek/deepseek-chat"
```

再次运行，

```python
from litellm import completion

response = completion(
    model=chat_model, 
    messages=[{ "content": "你有什么技能？","role": "user"}], 
    api_base=base_url,
    api_key=api_key
)
print(response.choices[0].message.content)
```

输出以下信息：
```markdown
作为一个人工智能助手，我拥有多种技能，可以帮助你处理各种任务！以下是我的一些核心能力：  

### 📚 **知识问答**  
- 提供科学、历史、技术、文化等领域的知识。  
- 解释复杂概念（如量子力学、经济学等）。  
- 实时查询最新信息（需联网）。  

### ✍️ **写作与创作**  
- 撰写文章、报告、论文大纲、商业计划书等。  
- 生成故事、诗歌、广告文案、社交媒体帖子。  
- 帮助润色、改写或优化文本。  

### 📊 **学习与工作辅助**  
- 代码编写与调试（Python、Java、SQL等）。  
- 数学计算、数据分析、公式推导。  
- 制作简历、求职信、PPT大纲。  

### 🌍 **语言翻译与学习**  
- 多语言翻译（中英、法、德、日、西等）。  
- 语法检查、单词解释、语言学习建议。  

### 🤖 **AI 与技术**  
- 解释机器学习、深度学习、AI 相关概念。  
- 提供编程示例（如爬虫、自动化脚本）。  
...
- 支持文件上传（PDF/Word/Excel等），提取并分析内容。  
- 联网搜索最新信息（需用户开启）。  

如果你有任何具体需求，可以直接告诉我，我会尽力帮你高效解决！ 😊
```


我们再继续搭建OpenAI Agent框架。

```python
from agents import Agent, Runner, set_tracing_disabled
from agents.extensions.models.litellm_model import LitellmModel
set_tracing_disabled(disabled=True)
llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)
agent = Agent(name="Assistant", model=llm, instructions="You are a helpful assistant")
result = Runner.run_sync(agent, "给我讲个程序员相亲的笑话")
print(result.final_output)
```

上面这段代码是不能在jupyter notebook中运行的，因为jupyter notebook不支持asyncio。会报错：
RuntimeError: This event loop is already running

要想在jupyter notebook中运行，需要做一些小小的改动

result = Runner.run_sync(agent, "给我讲个程序员相亲的笑话")
改成
result = await Runner.run(agent, "给我讲个程序员相亲的笑话")

就可以了。

建立try.py文件，终端运行 python try.py,输出：
```markdown
程序员小王去相亲，女生问他：“你是做什么工作的？”

小王：“我是程序员，专门写代码的。”

女生点点头：“哦……那你们是不是经常加班啊？”

小王认真回答：“其实我们用的是**敏捷开发**，加班不是常态，除非遇到**死线（Deadline）**。”

女生又问：“那你平时有什么爱好吗？”

小王：“我喜欢**重构代码**，偶尔**debug到凌晨**，周末爱逛**GitHub**和**Stack Overflow**。”

女生尴尬一笑：“……你谈过几次恋爱？”

小王想了想：“零次。不过如果恋爱像写代码，我现在应该算在**长期维护一个开源项目**，只是还没找到**合适的贡献者**。”

女生忍不住问：“你觉得我怎么样？”

小王盯着她看了三秒：“你的**兼容性**很好，但我们需要先跑个**单元测试**，再考虑是否**合并到主分支**。”

女生：“……再见。”

小王自言自语：“咦？怎么又**连接超时**了……”

---

（程序员：不是我不浪漫，是你们不懂我的**语法糖**😂）
```

好啦，至此我们已经成功搭建了OpenAI Agent框架，并用DeepSeek模型代替了原来的OpenAI模型。


## 配置其他模型

有多种方法可以配置模型，除了 litellm 提供的 [OpenAI 兼容端点](https://docs.litellm.ai/docs/providers/openai_compatible), 还可通过 openai 包来配置模型。

在Agent上指定智谱的模型，同步运行。

```python
from agents import Agent, Runner, set_tracing_disabled, set_default_openai_api, set_default_openai_client
from openai import AsyncOpenAI
import os
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()

chat_model = "glm-4-flash"
client = AsyncOpenAI(
    base_url="https://open.bigmodel.cn/api/paas/v4/",
    api_key=os.getenv('zhipu_key'),
)
set_default_openai_client(client=client, use_for_tracing=False)
set_default_openai_api("chat_completions")
set_tracing_disabled(disabled=True)

agent = Agent(name="Assistant", model=chat_model, instructions="You are a helpful assistant")
result = Runner.run_sync(agent, "给我讲个程序员相亲的笑话")
print(result.final_output)
```

输出如下：
有一天，一位程序员去相亲，女方问他：“你平时喜欢做什么消遣？”程序员想了想，回答道：“我平时喜欢编程。”

女方好奇地问：“哦？那你能给我讲一个编程中的笑话吗？”

程序员点点头，说道：“当然可以。有一天，我正在写一个程序，突然发现了一个bug。我检查了半天，最后发现是变量名写错了。于是，我修改了变量名
，bug就消失了。”

女方听了，疑惑地问：“那不是很简单吗？”

程序员笑着回答：“对啊，简单得就像你看到我单身了这么多年，还以为我找不到女朋友一样简单。”

如果要设置mistral的模型，可以改这几行代码：
```python
chat_model = "mistral-small-latest"
client = AsyncOpenAI(
    base_url="https://api.mistral.ai/v1",
    api_key=os.getenv('mistral_key'),
)
```

通过指定运行的模型来配置：

```python
from agents import (
    Agent,
    Model,
    ModelProvider,
    AsyncOpenAI,
    OpenAIChatCompletionsModel,
    RunConfig,
    Runner,
    set_tracing_disabled,
)
import os
import asyncio
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()
# 从环境变量中读取api_key
chat_model = "glm-4-flash"
client = AsyncOpenAI(
    base_url="https://open.bigmodel.cn/api/paas/v4/",
    api_key=os.getenv('zhipu_key'),
)


class CustomModelProvider(ModelProvider):
    def get_model(self) -> Model:
        return OpenAIChatCompletionsModel(model=chat_model, openai_client=client)


CUSTOM_MODEL_PROVIDER = CustomModelProvider()

set_tracing_disabled(disabled=True)


agent = Agent(
    name="Assistant", 
    model=OpenAIChatCompletionsModel(model=chat_model, openai_client=client),
    instructions="You are a helpful assistant")
def main():
    result = Runner.run_sync(agent, "外观设计专利保护期多少年？", run_config=RunConfig(model_provider=CUSTOM_MODEL_PROVIDER),)
    print(result.final_output)
if __name__ == "__main__":
    main()
```

输出：
在中国，外观设计专利的保护期限为十年，自申请之日起计算。这意味着一旦外观设计专利被授权，专利权人将在接下来的十年内享有对该外观设计的独
占使用权。如果专利权人希望继续保护其外观设计，可以在专利到期前六个月内申请续展，每次续展的期限也是十年。





以这种方式配置 智谱、deepseek、和mistral都不支持异步运行，需要用Runner.run_sync 去运行。如果用Runner.run 来运行，会报错：
Exception ignored in: <function _ProactorBasePipeTransport.__del__ at 0x00000149A89A5E10>
Traceback (most recent call last):
  File "D:\Python310\lib\asyncio\proactor_events.py", line 116, in __del__
  File "D:\Python310\lib\asyncio\proactor_events.py", line 108, in close
  File "D:\Python310\lib\asyncio\base_events.py", line 750, in call_soon
  File "D:\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
RuntimeError: Event loop is closed


## 支持异步运行的配置方法

我们可以借助openai官方提供的LitellmModel配合qwen，可以支持异步运行。

```python
from agents import Agent, Runner, set_tracing_disabled
from agents.extensions.models.litellm_model import LitellmModel
import os
import asyncio
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()
# 从环境变量中读取dashscope_api_key
dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')
dashscope_base_url = 'https://dashscope.aliyuncs.com/compatible-mode/v1'
dashscope_model = "dashscope/qwen-turbo"
llm = LitellmModel(model=dashscope_model, api_key=dashscope_api_key, base_url=dashscope_base_url)
agent = Agent(name="Assistant", model=llm, instructions="You are a helpful assistant")
async def main():
    result = await Runner.run(agent, "给我讲个程序员相亲的笑话")
    print(result.final_output)
if __name__ == "__main__":
    asyncio.run(main())
```

输出结果：
好的，这里有一个关于程序员相亲的笑话，希望能让你会心一笑：

---

一位程序员去相亲，见面后女生问他：“你平时都做些什么呀？”

程序员回答：“我主要写代码。”

女生又问：“那你写的代码都干嘛用呢？”

程序员想了想，说：“比如，我写了一个程序，可以让你的手机自动回复‘我在开会，稍后回你’。”

女生听了，若有所思地问：“那如果我回复‘我在开会，稍后回你’，你会怎么做？”

程序员愣了一下，然后说：“我会写一个程序，自动回复‘我在开会，稍后回你’。”

女生笑了，说：“看来我们挺合适的，你也可以帮我写个程序，让我自动回复‘我在开会，稍后回你’。”

程序员开心地笑了，说：“好的，我们可以一起写个程序，让我们的手机自动回复‘我在开会，稍后回你’。”

---

希望这个笑话能让你会心一笑！如果你有其他问题，欢迎随时提问。



这样配置就没有报错信息了。

如需进一步配置智能体使用的模型，可给Agent的参数model_settings传入 ModelSettings，该设置包含温度值等可选模型配置参数。

```python
from agents import Agent, ModelSettings

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model_settings=ModelSettings(temperature=0.1),
)
```


日志配置

SDK内置两个未配置处理器的Python日志记录器。默认情况下，仅警告和错误信息会输出至stdout，其他日志将被抑制。

如需启用详细日志输出，请使用enable_verbose_stdout_logging()函数。

```python
from agents import enable_verbose_stdout_logging
enable_verbose_stdout_logging()
```

您也可以通过添加处理器、过滤器、格式化器等自定义日志行为，更多细节请参阅Python日志指南。

```python
import logging

logger = logging.getLogger("openai.agents") # or openai.agents.tracing for the Tracing logger

# To make all logs show up
logger.setLevel(logging.DEBUG)
# To make info and above show up
logger.setLevel(logging.INFO)
# To make warning and above show up
logger.setLevel(logging.WARNING)
# etc

# You can customize this as needed, but this will output to `stderr` by default
logger.addHandler(logging.StreamHandler())
```

日志中的敏感数据
部分日志可能包含敏感信息（例如用户数据）。如需禁用此类数据记录，请设置以下环境变量：

禁用大模型输入输出日志记录：
export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1


禁用工具输入输出日志记录：
export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1
